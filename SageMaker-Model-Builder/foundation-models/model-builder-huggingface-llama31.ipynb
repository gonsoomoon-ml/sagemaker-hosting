{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0d57a8",
   "metadata": {},
   "source": [
    "# Llama 3.1 Hugging Face Hub - ModelBuilder\n",
    "\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g5.2xl` with a 100GB EBS volume attached. If you are not using local mode, feel free to make use of a smaller CPU instance type / EBS volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27e9a7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install boto3 sagemaker -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ef72e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker                     2.228.0\n",
      "sagemaker_pyspark             1.4.5\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974aacb",
   "metadata": {},
   "source": [
    "# SageMaker Model Builder experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481b830-e1ae-459e-8549-32384c4bbe80",
   "metadata": {},
   "source": [
    "In this example, we are using ModelBuilder to deploy an Llama 2 model directly. You can use `Mode` to switch between local testing and deploying to a SageMaker Endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d61a15-53e1-4687-a15c-69d6e3b695e3",
   "metadata": {},
   "source": [
    "### SageMaker ModelBuilder: Local deployment\n",
    "\n",
    "Now we will use SageMaker ModelBuilder class to prepare the model for local and remote deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d643065-0f31-4da4-a472-2c4e208cfd48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/SageMaker/.xdg/config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "import os\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# get execution role\n",
    "# please use execution role if you are using notebook instance or update the role arn if you are using a different role\n",
    "execution_role = get_execution_role() if get_execution_role() is not None else \"your-role-arn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60004f1-97b7-4c63-b784-89515292f58c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sagemaker-hosting/SageMaker-Model-Builder/foundation-models/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), 'Llama-3.1-8B-Instruct') \n",
    "os.makedirs(path, exist_ok=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3e702-deac-4890-8d76-7b2d5cb9b572",
   "metadata": {},
   "source": [
    "[Llama 2 is a gated model](https://huggingface.co/meta-llama/Llama-2-7b-hf) and requires access to be approved. Once approved you will need to pass your [Hugging Face access token](https://huggingface.co/docs/hub/security-tokens) as seen in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa818577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    " \n",
    "is_sagemaker_notebook = True\n",
    "# is_sagemaker_notebook = False # use VS Code\n",
    "\n",
    "if is_sagemaker_notebook:\n",
    "    HF_TOKEN = getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN') or getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "    print(\"token: \", HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3351efc2-cc4e-4a6e-ad0c-bf2c6ab87953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.serve import Mode\n",
    "import json\n",
    "\n",
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "response = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the east coast.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {}\n",
    "}\n",
    "\n",
    "sample_output = [\n",
    "    {\n",
    "        \"generated_text\": response\n",
    "    }\n",
    "]\n",
    "\n",
    "model_builder = ModelBuilder(\n",
    "    # model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    model = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    schema_builder=SchemaBuilder(sample_input, sample_output),\n",
    "    model_path=path, #local path where artifacts will be saved\n",
    "    mode=Mode.LOCAL_CONTAINER, # The model will be deployed locally. Change to Mode.SAGEMAKER_ENDPOINT to deploy to a SageMaker endpoint. \n",
    "    env_vars={\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": HF_TOKEN # Llama 2 is a gated model and requires a Hugging Face Hub token. \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e23e5d-c863-4528-a3b8-ec83cd6889e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "ModelBuilder: WARNING:     HuggingFace Model ID support on model server: None is not currently supported. Defaulting to TGI\n",
      "ModelBuilder: INFO:     Local instance_type ml.g5.12xlarge detected. ml.g5.12xlarge will be default when deploying to a SageMaker Endpoint. This default can be overriden in model.deploy()\n",
      "ModelBuilder: WARNING:     91.85528052148224 percent of docker disk space at /var/lib/docker is used. Please consider freeing up disk space or increasing the EBS volume if you are on a SageMaker Notebook.\n",
      "ModelBuilder: INFO:     CUDA enabled hardware on the device: ['NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB']\n",
      "ModelBuilder: INFO:     Max GPU parallelism of 4 is allowed. Total attention heads 32\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py310\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n",
      "ModelBuilder: INFO:     Auto detected 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0. Proceeding with the the deployment.\n"
     ]
    }
   ],
   "source": [
    "model = model_builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223e930-598a-4552-8341-4866b2ebf7e1",
   "metadata": {},
   "source": [
    "### Tune the model container locally to get the best configuration for deployment\n",
    "\n",
    "A neat feature of Model Builder is the ability to run local tuning of the container parameter(s) when you use the LOCAL_CONTAINER mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f46a639-7fc5-43b8-a61e-ed8dbb042980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: INFO:     CUDA enabled hardware on the device: ['NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB']\n",
      "ModelBuilder: INFO:     Model can be sharded across [4, 2, 1] GPUs\n",
      "ModelBuilder: INFO:     Trying num shard: 4, dtype: bfloat16...\n",
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: INFO:     Pulling image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0 from repository...\n",
      "ModelBuilder: INFO:     Waiting for model server TGI to start up...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tuned_model = model.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58184f43-61b4-4299-b873-bc2cf6ff5fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: INFO:     Pulling image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0 from repository...\n",
      "ModelBuilder: DEBUG:     Stopping currently running container...\n",
      "ModelBuilder: INFO:     Waiting for model server TGI to start up...\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559651Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Args {\n",
      "ModelBuilder: DEBUG:         model_id: \"meta-llama/Llama-2-7b-hf\",\n",
      "ModelBuilder: DEBUG:         revision: None,\n",
      "ModelBuilder: DEBUG:         validation_workers: 2,\n",
      "ModelBuilder: DEBUG:         sharded: Some(\n",
      "ModelBuilder: DEBUG:             true,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         num_shard: Some(\n",
      "ModelBuilder: DEBUG:             4,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         quantize: None,\n",
      "ModelBuilder: DEBUG:         speculate: None,\n",
      "ModelBuilder: DEBUG:         dtype: Some(\n",
      "ModelBuilder: DEBUG:             BFloat16,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         trust_remote_code: false,\n",
      "ModelBuilder: DEBUG:         max_concurrent_requests: 128,\n",
      "ModelBuilder: DEBUG:         max_best_of: 2,\n",
      "ModelBuilder: DEBUG:         max_stop_sequences: 4,\n",
      "ModelBuilder: DEBUG:         max_top_n_tokens: 5,\n",
      "ModelBuilder: DEBUG:         max_input_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_input_length: None,\n",
      "ModelBuilder: DEBUG:         max_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         waiting_served_ratio: 0.3,\n",
      "ModelBuilder: DEBUG:         max_batch_prefill_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_batch_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_waiting_tokens: 20,\n",
      "ModelBuilder: DEBUG:         max_batch_size: None,\n",
      "ModelBuilder: DEBUG:         cuda_graphs: None,\n",
      "ModelBuilder: DEBUG:         hostname: \"ip-172-16-34-227.ec2.internal\",\n",
      "ModelBuilder: DEBUG:         port: 8080,\n",
      "ModelBuilder: DEBUG:         shard_uds_path: \"/tmp/text-generation-server\",\n",
      "ModelBuilder: DEBUG:         master_addr: \"localhost\",\n",
      "ModelBuilder: DEBUG:         master_port: 29500,\n",
      "ModelBuilder: DEBUG:         huggingface_hub_cache: Some(\n",
      "ModelBuilder: DEBUG:             \"/opt/ml/model/\",\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         weights_cache_override: None,\n",
      "ModelBuilder: DEBUG:         disable_custom_kernels: false,\n",
      "ModelBuilder: DEBUG:         cuda_memory_fraction: 1.0,\n",
      "ModelBuilder: DEBUG:         rope_scaling: None,\n",
      "ModelBuilder: DEBUG:         rope_factor: None,\n",
      "ModelBuilder: DEBUG:         json_output: false,\n",
      "ModelBuilder: DEBUG:         otlp_endpoint: None,\n",
      "ModelBuilder: DEBUG:         otlp_service_name: \"text-generation-inference.router\",\n",
      "ModelBuilder: DEBUG:         cors_allow_origin: [],\n",
      "ModelBuilder: DEBUG:         watermark_gamma: None,\n",
      "ModelBuilder: DEBUG:         watermark_delta: None,\n",
      "ModelBuilder: DEBUG:         ngrok: false,\n",
      "ModelBuilder: DEBUG:         ngrok_authtoken: None,\n",
      "ModelBuilder: DEBUG:         ngrok_edge: None,\n",
      "ModelBuilder: DEBUG:         tokenizer_config_path: None,\n",
      "ModelBuilder: DEBUG:         disable_grammar_support: false,\n",
      "ModelBuilder: DEBUG:         env: false,\n",
      "ModelBuilder: DEBUG:         max_client_batch_size: 4,\n",
      "ModelBuilder: DEBUG:         lora_adapters: None,\n",
      "ModelBuilder: DEBUG:         disable_usage_stats: false,\n",
      "ModelBuilder: DEBUG:         disable_crash_reports: false,\n",
      "ModelBuilder: DEBUG:     }\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559721Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559775Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_input_tokens` to 4095\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559788Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_total_tokens` to 4096\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559790Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_batch_prefill_tokens` to 4145\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559793Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using default cuda graphs [1, 2, 4, 8, 16, 32]\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559796Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Sharding model on 4 processes\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559876Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting check and download process for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:14.606520Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Files are already present on the host. Skipping download.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.263619Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Successfully downloaded weights for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.263828Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.263861Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.264807Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.264919Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.951367Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-0\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.951410Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-1\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.960559Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-3\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.974660Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.708372387s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.975001Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.708575449s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.975014Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.708197365s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.015984Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-2\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.074846Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.808401807s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.171097Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting Webserver\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.235297Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m228:\u001b[0m Using the Hugging Face API\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.235345Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m \u001b[2m/usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/hf-hub-0.3.2/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m55:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.422102Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m577:\u001b[0m Serving revision 01c7f73d771dfac7d292323805ebc428287df4f9 of model meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.471650Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m342:\u001b[0m Overriding LlamaTokenizer with TemplateProcessing to follow python override defined in https://github.com/huggingface/transformers/blob/4aa17d00690b7f82c95bb2949ea57e22c35b4336/src/transformers/models/llama/tokenization_llama_fast.py#L203-L205\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.471679Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m357:\u001b[0m Using config Some(Llama)\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.471686Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m384:\u001b[0m Invalid hostname, defaulting to 0.0.0.0\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.634265Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1572:\u001b[0m Warming up model\n",
      "ModelBuilder: DEBUG:     Container logging done. All container logs processed.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:26.897330Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Cuda Graphs are enabled for sizes [32, 16, 8, 4, 2, 1]\n",
      "ModelBuilder: DEBUG:     Ping health check has passed. Returned [{'generated_text': ' eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\\nThe diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The shell is oval in shape, with a high, domed carapace and a flattened plastron. The carapace is dark brown to black, with a yellowish-brown to orange-brown stripe running down the middle of each scute. The plastron is yellowish-'}]\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "local_predictor = tuned_model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d569f5d8-0185-496a-bd8f-1a5835db8da2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the',\n",
       " 'parameters': {'max_new_tokens': 128}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_sample_input = model_builder.schema_builder.sample_input\n",
    "updated_sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ef5548-2178-48c4-83e5-6e808be1ce30",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\\nThe diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The shell is oval in shape, with a high, domed carapace and a flattened plastron. The carapace is dark brown to black, with a yellowish-brown to orange-brown stripe running down the middle of each scute. The plastron is yellowish-'}]\n",
      "CPU times: user 4.08 ms, sys: 0 ns, total: 4.08 ms\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(local_predictor.predict(updated_sample_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e1f8d-3bec-4c82-a514-fa84daaf39ee",
   "metadata": {},
   "source": [
    "### SageMaker ModelBuilder: Deploy to a SageMaker Endpoint\n",
    "\n",
    "Now we have tested the model prediction locally, we can continue to deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f2edb6f-341f-4c3d-98c4-3f642c9d4efa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: WARNING:     Deploying in SAGEMAKER_ENDPOINT Mode, overriding existing configurations set for LOCAL_CONTAINER mode\n",
      "ModelBuilder: DEBUG:     Uploading TGI Model Resources uncompressed to: s3://sagemaker-us-east-1-057716757052/huggingface-pytorch-tgi-inference-2024-08-11-03-10-46-380/code\n",
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2024-08-11-03-15-02-850\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574\n",
      "WARNING:sagemaker:Failed to enable live logging: An error occurred (AccessDeniedException) when calling the FilterLogEvents operation: User: arn:aws:sts::057716757052:assumed-role/gen_ai_gsmoon/SageMaker is not authorized to perform: logs:FilterLogEvents on resource: arn:aws:logs:us-east-1:057716757052:log-group:/aws/sagemaker/Endpoints/huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574:log-stream: because no identity-based policy allows the logs:FilterLogEvents action. Fallback to default logging...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "predictor = tuned_model.deploy(mode=Mode.SAGEMAKER_ENDPOINT, role=execution_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "174f97c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the',\n",
       " 'parameters': {'max_new_tokens': 128}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec8eabf8-0563-4d3c-9b14-0f13785fdad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\n",
      "The diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The shell is oval in shape, with a high, rounded keel running down the middle of the back. The shell is dark brown to black, with a yellowish-orange stripe running down the middle of the back. The head is small and triangular in shape, with a pointed snout. The eyes are\n",
      "CPU times: user 11.6 ms, sys: 1.66 ms, total: 13.2 ms\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(predictor.predict(updated_sample_input)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8f0fe",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68053e21-a69d-4b1d-8a64-7d3f13e53cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-tgi-inference-2024-08-11-03-15-02-850\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574\n"
     ]
    }
   ],
   "source": [
    "local_predictor.delete_predictor()\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3e748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('pytorch_p310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2257b1c3513dc4782645ad49f694a4b0012bebbbbc3534a56d350db8e4f89a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
