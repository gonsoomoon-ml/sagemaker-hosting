{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0d57a8",
   "metadata": {},
   "source": [
    "# Llama 2 Hugging Face Hub - ModelBuilder\n",
    "\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g5.2xl` with a 100GB EBS volume attached. If you are not using local mode, feel free to make use of a smaller CPU instance type / EBS volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27e9a7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3 sagemaker -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ef72e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker                     2.228.0\n",
      "sagemaker_pyspark             1.4.5\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "435bd235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974aacb",
   "metadata": {},
   "source": [
    "# SageMaker Model Builder experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b91159",
   "metadata": {},
   "source": [
    "In the new experience, we have introduced a few new constructs. Here we will focus on the following: \n",
    "\n",
    "1. ModelBuilder\n",
    "2. SchemaBuilder\n",
    "3. InferenceSpec\n",
    "\n",
    "In the following section, we will define these constructs and provide examples to elaborate on each one.\n",
    "\n",
    "4.1 ModelBuilder:\n",
    "\n",
    "ModelBuilder is a Python class that takes a framework model (such as XGBoost or PyTorch) or an Inference Spec (more details below) and converts them into a SageMaker deployable model. ModelBuilder provides a `build` function that generates the artifacts for deployment. The model artifact generated is specific to the model server, which is also customizable as one of the inputs.\n",
    "\n",
    "```python\n",
    "Class definition:\n",
    "\n",
    "class ModelBuilder(\n",
    "    model_path: str | None = '/tmp/sagemaker/model-builder/' + uuid.uuid1().hex,\n",
    "    role_arn: str | None = None,\n",
    "    sagemaker_session: Session | None = None,\n",
    "    name: str | None = 'model-name-' + uuid.uuid1().hex,\n",
    "    mode: Mode | None = Mode.SAGEMAKER_ENDPOINT,\n",
    "    shared_libs: List[str] = lambda : [],\n",
    "    dependencies: Dict[str, Any] | None = lambda : { \"auto\": False },\n",
    "    env_vars: Dict[str, str] | None = lambda : {},\n",
    "    log_level: int | None = logging.DEBUG,\n",
    "    content_type: str | None = None,\n",
    "    accept_type: str | None = None,\n",
    "    s3_model_data_url: str | None = None,\n",
    "    instance_type: str | None = \"ml.c5.xlarge\",\n",
    "    schema_builder: str | None = None,\n",
    "    model: Any | None = None,\n",
    "    inference_spec: InferenceSpec = None,\n",
    "    image_uri: str | None = None,\n",
    "    model_server: str | None = None\n",
    ")\n",
    "```\n",
    "Example:\n",
    "\n",
    "The above class file provide all the options for customization. However to deploy the framework model, the model builder just expects model, input, output and the role. \n",
    "\n",
    "```python\n",
    "model_builder = ModelBuilder(\n",
    "    model=model,  # Pass in the actual model object. It's \"predict\" method will be invoked in the endpoint.\n",
    "    schema_builder=SchemaBuilder(input, output), # Pass in a \"SchemaBuilder\" which will use the sample test input and output objects to infer the serialization needed.\n",
    "    role_arn=role, # Pass in the role arn or update intelligent defaults.\n",
    "    )\n",
    "```\n",
    "\n",
    "4.2 SchemaBuilder:\n",
    "\n",
    "The SchemaBuilder enables you to define the input and output for your endpoint. It allows the SchemaBuilder to generate the corresponding marshalling functions for serializing and deserializing the input and output. For further details, please consult the notebook or refer to the video.\n",
    "\n",
    "Class definition:\n",
    "```python\n",
    "class SchemaBuilder(\n",
    "    sample_input: Any,\n",
    "    sample_output: Any,\n",
    "    input_translator: CustomPayloadTranslator = None,\n",
    "    output_translator: CustomPayloadTranslator = None\n",
    ")\n",
    "```\n",
    "Example:\n",
    "\n",
    "The CustomPayloadTranslator class provides all the options for customization. However, for [common inference data format](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html), you can just provide the sample input/output for the SchemaBuilder.\n",
    "```python\n",
    "input = \"How is the demo going?\"\n",
    "output = \"Comment la démo va-t-elle?\"\n",
    "schema = SchemaBuilder(input, output)\n",
    "```\n",
    "\n",
    "4.3 InferenceSpec\n",
    "\n",
    "In the case you want to specify custom function to load and invoke the model instead of the framework model function, then you can pass the inference spec with your implementation in `load` and `invoke` function. \n",
    "\n",
    "class definition:\n",
    "```python\n",
    "class InferenceSpec(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def load(self, model_dir: str):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def invoke(self, input_object: object, model: object):\n",
    "        pass\n",
    "```\n",
    "Example:\n",
    "```python\n",
    "class MyInferenceSpec(InferenceSpec):\n",
    "    def load(self, model_dir: str):\n",
    "        return pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "        \n",
    "    def invoke(self, input, model):\n",
    "        return model(input)\n",
    "   \n",
    "inf_spec = MyInferenceSpec()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481b830-e1ae-459e-8549-32384c4bbe80",
   "metadata": {},
   "source": [
    "In this example, we are using ModelBuilder to deploy an Llama 2 model directly. You can use `Mode` to switch between local testing and deploying to a SageMaker Endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d61a15-53e1-4687-a15c-69d6e3b695e3",
   "metadata": {},
   "source": [
    "### SageMaker ModelBuilder: Local deployment\n",
    "\n",
    "Now we will use SageMaker ModelBuilder class to prepare the model for local and remote deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d643065-0f31-4da4-a472-2c4e208cfd48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/SageMaker/.xdg/config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "import os\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# get execution role\n",
    "# please use execution role if you are using notebook instance or update the role arn if you are using a different role\n",
    "execution_role = get_execution_role() if get_execution_role() is not None else \"your-role-arn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f60004f1-97b7-4c63-b784-89515292f58c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/sagemaker-hosting/SageMaker-Model-Builder/foundation-models/llama2-7b\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), 'llama2-7b') \n",
    "os.makedirs(path, exist_ok=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3e702-deac-4890-8d76-7b2d5cb9b572",
   "metadata": {},
   "source": [
    "[Llama 2 is a gated model](https://huggingface.co/meta-llama/Llama-2-7b-hf) and requires access to be approved. Once approved you will need to pass your [Hugging Face access token](https://huggingface.co/docs/hub/security-tokens) as seen in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de536cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "# is_sagemaker_notebook = False\n",
    "\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    key_val = \"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\"    \n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "# !huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3351efc2-cc4e-4a6e-ad0c-bf2c6ab87953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.serve import Mode\n",
    "import json\n",
    "\n",
    "prompt = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the\"\n",
    "response = \"The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the east coast.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {}\n",
    "}\n",
    "\n",
    "sample_output = [\n",
    "    {\n",
    "        \"generated_text\": response\n",
    "    }\n",
    "]\n",
    "\n",
    "model_builder = ModelBuilder(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    # model = \"meta-llama/Meta-Llama-3-8B\",\n",
    "    schema_builder=SchemaBuilder(sample_input, sample_output),\n",
    "    model_path=path, #local path where artifacts will be saved\n",
    "    mode=Mode.LOCAL_CONTAINER, # The model will be deployed locally. Change to Mode.SAGEMAKER_ENDPOINT to deploy to a SageMaker endpoint. \n",
    "    env_vars={\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": HF_TOKEN # Llama 2 is a gated model and requires a Hugging Face Hub token. \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1a1d1-572c-436e-8b6a-d9378b1e3679",
   "metadata": {},
   "source": [
    "By default, ModelBuilder will use [TGI](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers) container as the underlying container for Hugging Face models. In case you would like to use the [LMI containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers), you can configure the ModelBuilder as follow:\n",
    "\n",
    "```python\n",
    "from sagemaker.serve import ModelServer\n",
    "\n",
    "model_builder = ModelBuilder(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    schema_builder=SchemaBuilder(sample_input, sample_output),\n",
    "    model_path=path, #local path where artifacts will be saved\n",
    "    mode=Mode.LOCAL_CONTAINER, # The model will be deployed locally. Change to Mode.SAGEMAKER_ENDPOINT to deploy to a SageMaker endpoint. \n",
    "    model_server=ModelServer.DJL_SERVING,\n",
    "    env_vars={\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": \"<YourHuggingFaceToken>\" # Llama 2 is a gated model and requires a Hugging Face Hub token. \n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e23e5d-c863-4528-a3b8-ec83cd6889e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "ModelBuilder: INFO:     Either inference spec or model is provided. ModelBuilder is not handling MLflow model input\n",
      "ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "ModelBuilder: WARNING:     HuggingFace JumpStart Model ID not detected. Building for HuggingFace Model ID.\n",
      "ModelBuilder: WARNING:     HuggingFace Model ID support on model server: None is not currently supported. Defaulting to TGI\n",
      "ModelBuilder: INFO:     Local instance_type ml.g5.12xlarge detected. ml.g5.12xlarge will be default when deploying to a SageMaker Endpoint. This default can be overriden in model.deploy()\n",
      "ModelBuilder: WARNING:     91.78414294794862 percent of docker disk space at /var/lib/docker is used. Please consider freeing up disk space or increasing the EBS volume if you are on a SageMaker Notebook.\n",
      "ModelBuilder: INFO:     CUDA enabled hardware on the device: ['NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB']\n",
      "ModelBuilder: INFO:     Max GPU parallelism of 4 is allowed. Total attention heads 32\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py310\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n",
      "ModelBuilder: INFO:     Auto detected 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0. Proceeding with the the deployment.\n"
     ]
    }
   ],
   "source": [
    "model = model_builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223e930-598a-4552-8341-4866b2ebf7e1",
   "metadata": {},
   "source": [
    "### Tune the model container locally to get the best configuration for deployment\n",
    "\n",
    "A neat feature of Model Builder is the ability to run local tuning of the container parameter(s) when you use the LOCAL_CONTAINER mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f46a639-7fc5-43b8-a61e-ed8dbb042980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: INFO:     CUDA enabled hardware on the device: ['NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB', 'NVIDIA A10G, 22723 MiB']\n",
      "ModelBuilder: INFO:     Model can be sharded across [4, 2, 1] GPUs\n",
      "ModelBuilder: INFO:     Trying num shard: 4, dtype: bfloat16...\n",
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: INFO:     Pulling image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0 from repository...\n",
      "ModelBuilder: INFO:     Waiting for model server TGI to start up...\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:19.516656Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Args {\n",
      "ModelBuilder: DEBUG:         model_id: \"meta-llama/Llama-2-7b-hf\",\n",
      "ModelBuilder: DEBUG:         revision: None,\n",
      "ModelBuilder: DEBUG:         validation_workers: 2,\n",
      "ModelBuilder: DEBUG:         sharded: Some(\n",
      "ModelBuilder: DEBUG:             true,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         num_shard: Some(\n",
      "ModelBuilder: DEBUG:             4,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         quantize: None,\n",
      "ModelBuilder: DEBUG:         speculate: None,\n",
      "ModelBuilder: DEBUG:         dtype: Some(\n",
      "ModelBuilder: DEBUG:             BFloat16,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         trust_remote_code: false,\n",
      "ModelBuilder: DEBUG:         max_concurrent_requests: 128,\n",
      "ModelBuilder: DEBUG:         max_best_of: 2,\n",
      "ModelBuilder: DEBUG:         max_stop_sequences: 4,\n",
      "ModelBuilder: DEBUG:         max_top_n_tokens: 5,\n",
      "ModelBuilder: DEBUG:         max_input_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_input_length: None,\n",
      "ModelBuilder: DEBUG:         max_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         waiting_served_ratio: 0.3,\n",
      "ModelBuilder: DEBUG:         max_batch_prefill_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_batch_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_waiting_tokens: 20,\n",
      "ModelBuilder: DEBUG:         max_batch_size: None,\n",
      "ModelBuilder: DEBUG:         cuda_graphs: None,\n",
      "ModelBuilder: DEBUG:         hostname: \"ip-172-16-34-227.ec2.internal\",\n",
      "ModelBuilder: DEBUG:         port: 8080,\n",
      "ModelBuilder: DEBUG:         shard_uds_path: \"/tmp/text-generation-server\",\n",
      "ModelBuilder: DEBUG:         master_addr: \"localhost\",\n",
      "ModelBuilder: DEBUG:         master_port: 29500,\n",
      "ModelBuilder: DEBUG:         huggingface_hub_cache: Some(\n",
      "ModelBuilder: DEBUG:             \"/opt/ml/model/\",\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         weights_cache_override: None,\n",
      "ModelBuilder: DEBUG:         disable_custom_kernels: false,\n",
      "ModelBuilder: DEBUG:         cuda_memory_fraction: 1.0,\n",
      "ModelBuilder: DEBUG:         rope_scaling: None,\n",
      "ModelBuilder: DEBUG:         rope_factor: None,\n",
      "ModelBuilder: DEBUG:         json_output: false,\n",
      "ModelBuilder: DEBUG:         otlp_endpoint: None,\n",
      "ModelBuilder: DEBUG:         otlp_service_name: \"text-generation-inference.router\",\n",
      "ModelBuilder: DEBUG:         cors_allow_origin: [],\n",
      "ModelBuilder: DEBUG:         watermark_gamma: None,\n",
      "ModelBuilder: DEBUG:         watermark_delta: None,\n",
      "ModelBuilder: DEBUG:         ngrok: false,\n",
      "ModelBuilder: DEBUG:         ngrok_authtoken: None,\n",
      "ModelBuilder: DEBUG:         ngrok_edge: None,\n",
      "ModelBuilder: DEBUG:         tokenizer_config_path: None,\n",
      "ModelBuilder: DEBUG:         disable_grammar_support: false,\n",
      "ModelBuilder: DEBUG:         env: false,\n",
      "ModelBuilder: DEBUG:         max_client_batch_size: 4,\n",
      "ModelBuilder: DEBUG:         lora_adapters: None,\n",
      "ModelBuilder: DEBUG:         disable_usage_stats: false,\n",
      "ModelBuilder: DEBUG:         disable_crash_reports: false,\n",
      "ModelBuilder: DEBUG:     }\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:19.517263Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:19.568015Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_input_tokens` to 4095\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:19.568032Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_total_tokens` to 4096\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:19.568034Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_batch_prefill_tokens` to 4145\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:19.568036Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using default cuda graphs [1, 2, 4, 8, 16, 32]\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:19.568039Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Sharding model on 4 processes\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:19.568123Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting check and download process for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:22.639200Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Files are already present on the host. Skipping download.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:23.271855Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Successfully downloaded weights for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:23.272073Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:23.272079Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:23.272615Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:23.273233Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:33.283862Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:33.284190Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:33.284278Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:33.284355Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     Container logging done. All container logs processed.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:43.292938Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:43.292893Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:43.293143Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:53.301588Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:53.301605Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:53.301613Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:46:53.301991Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     Container logging done. All container logs processed.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:03.310026Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:03.310923Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:03.312846Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:13.322272Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:13.322304Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:13.322328Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:13.322258Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     Container logging done. All container logs processed.\n",
      "ModelBuilder: DEBUG:     Ping health check has passed. Returned [{'generated_text': ' eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\\nThe diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The shell is oval in shape, with a high, domed carapace and a flattened plastron. The carapace is dark brown to black, with a yellowish-brown to orange-brown stripe running down the middle of each scute. The plastron is yellowish-'}]\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     =============== Running Serial Benchmark.... ================\n",
      "ModelBuilder: INFO:     Invocation: 0 => Latency: 1.4346675029955804, Tokens/s: 54.7165108543215\n",
      "ModelBuilder: INFO:     Invocation: 1 => Latency: 1.4350166579242796, Tokens/s: 54.70319774095762\n",
      "ModelBuilder: INFO:     Invocation: 2 => Latency: 1.4352405039826408, Tokens/s: 54.6946660034822\n",
      "ModelBuilder: INFO:     Invocation: 3 => Latency: 1.4365825440036133, Tokens/s: 54.64357083250383\n",
      "ModelBuilder: INFO:     Invocation: 4 => Latency: 1.4841861020540819, Tokens/s: 52.890941298640165\n",
      "ModelBuilder: INFO:     Invocation: 5 => Latency: 1.4360991090070456, Tokens/s: 54.66196553403395\n",
      "ModelBuilder: INFO:     Invocation: 6 => Latency: 1.4357547840336338, Tokens/s: 54.67507465269297\n",
      "ModelBuilder: INFO:     Invocation: 7 => Latency: 1.4325278570177034, Tokens/s: 54.79823628939726\n",
      "ModelBuilder: INFO:     Invocation: 8 => Latency: 1.4334302979987115, Tokens/s: 54.76373710643485\n",
      "ModelBuilder: INFO:     Invocation: 9 => Latency: 1.4347032719524577, Tokens/s: 54.71514670289348\n",
      "ModelBuilder: INFO:     ================ Completed Serial Benchmark =================\n",
      "\n",
      "ModelBuilder: INFO:     ============= Running Concurrent Benchmark.... ==============\n",
      "ModelBuilder: INFO:     User: 0 => latency: 2.3543051710585132 seconds\n",
      "ModelBuilder: INFO:     User: 1 => latency: 2.353633573045954 seconds\n",
      "ModelBuilder: INFO:     User: 2 => latency: 2.3534142709104344 seconds\n",
      "ModelBuilder: INFO:     User: 3 => latency: 2.3514580480987206 seconds\n",
      "ModelBuilder: INFO:     User: 4 => latency: 2.3524166690185666 seconds\n",
      "ModelBuilder: INFO:     User: 5 => latency: 2.349224903038703 seconds\n",
      "ModelBuilder: INFO:     User: 6 => latency: 2.3473937310045585 seconds\n",
      "ModelBuilder: INFO:     User: 7 => latency: 2.3471541790058836 seconds\n",
      "ModelBuilder: INFO:     User: 8 => latency: 2.3470529780024663 seconds\n",
      "ModelBuilder: INFO:     User: 9 => latency: 2.341197288944386 seconds\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     Model Latencies for Queued Requests:\n",
      "ModelBuilder: INFO:     queued request 0: xxx\n",
      "ModelBuilder: INFO:     queued request 1: xxx\n",
      "ModelBuilder: INFO:     queued request 2: xxx\n",
      "ModelBuilder: INFO:     queued request 3: xxx\n",
      "ModelBuilder: INFO:     queued request 4: xxx\n",
      "ModelBuilder: INFO:     queued request 5: xxx\n",
      "ModelBuilder: INFO:     queued request 6: xxx\n",
      "ModelBuilder: INFO:     queued request 7: xxx\n",
      "ModelBuilder: INFO:     queued request 8: xxx\n",
      "ModelBuilder: INFO:     queued request 9: xxx\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     Concurrent Benchmark Metrics => throughput/s: 0.4255817023001885, standard deviation: 0.003902175776030024\n",
      "ModelBuilder: INFO:     ============== Completed Concurrent Benchmark ===============\n",
      "\n",
      "ModelBuilder: INFO:     Average latency: 1.4398208630969749, throughput/s: 0.4255817023001885 for configuration: {'HUGGING_FACE_HUB_TOKEN': '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', 'HF_TOKEN': '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', 'HF_MODEL_ID': 'meta-llama/Llama-2-7b-hf', 'SHARDED': 'true', 'NUM_SHARD': '4', 'DTYPE': 'bfloat16', 'MODEL_LOADING_TIMEOUT': '1800'}\n",
      "ModelBuilder: INFO:     Trying num shard: 2, dtype: bfloat16...\n",
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: INFO:     Pulling image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0 from repository...\n",
      "ModelBuilder: DEBUG:     Stopping currently running container...\n",
      "ModelBuilder: INFO:     Waiting for model server TGI to start up...\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:50.653314Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Args {\n",
      "ModelBuilder: DEBUG:         model_id: \"meta-llama/Llama-2-7b-hf\",\n",
      "ModelBuilder: DEBUG:         revision: None,\n",
      "ModelBuilder: DEBUG:         validation_workers: 2,\n",
      "ModelBuilder: DEBUG:         sharded: Some(\n",
      "ModelBuilder: DEBUG:             true,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         num_shard: Some(\n",
      "ModelBuilder: DEBUG:             2,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         quantize: None,\n",
      "ModelBuilder: DEBUG:         speculate: None,\n",
      "ModelBuilder: DEBUG:         dtype: Some(\n",
      "ModelBuilder: DEBUG:             BFloat16,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         trust_remote_code: false,\n",
      "ModelBuilder: DEBUG:         max_concurrent_requests: 128,\n",
      "ModelBuilder: DEBUG:         max_best_of: 2,\n",
      "ModelBuilder: DEBUG:         max_stop_sequences: 4,\n",
      "ModelBuilder: DEBUG:         max_top_n_tokens: 5,\n",
      "ModelBuilder: DEBUG:         max_input_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_input_length: None,\n",
      "ModelBuilder: DEBUG:         max_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         waiting_served_ratio: 0.3,\n",
      "ModelBuilder: DEBUG:         max_batch_prefill_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_batch_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_waiting_tokens: 20,\n",
      "ModelBuilder: DEBUG:         max_batch_size: None,\n",
      "ModelBuilder: DEBUG:         cuda_graphs: None,\n",
      "ModelBuilder: DEBUG:         hostname: \"ip-172-16-34-227.ec2.internal\",\n",
      "ModelBuilder: DEBUG:         port: 8080,\n",
      "ModelBuilder: DEBUG:         shard_uds_path: \"/tmp/text-generation-server\",\n",
      "ModelBuilder: DEBUG:         master_addr: \"localhost\",\n",
      "ModelBuilder: DEBUG:         master_port: 29500,\n",
      "ModelBuilder: DEBUG:         huggingface_hub_cache: Some(\n",
      "ModelBuilder: DEBUG:             \"/opt/ml/model/\",\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         weights_cache_override: None,\n",
      "ModelBuilder: DEBUG:         disable_custom_kernels: false,\n",
      "ModelBuilder: DEBUG:         cuda_memory_fraction: 1.0,\n",
      "ModelBuilder: DEBUG:         rope_scaling: None,\n",
      "ModelBuilder: DEBUG:         rope_factor: None,\n",
      "ModelBuilder: DEBUG:         json_output: false,\n",
      "ModelBuilder: DEBUG:         otlp_endpoint: None,\n",
      "ModelBuilder: DEBUG:         otlp_service_name: \"text-generation-inference.router\",\n",
      "ModelBuilder: DEBUG:         cors_allow_origin: [],\n",
      "ModelBuilder: DEBUG:         watermark_gamma: None,\n",
      "ModelBuilder: DEBUG:         watermark_delta: None,\n",
      "ModelBuilder: DEBUG:         ngrok: false,\n",
      "ModelBuilder: DEBUG:         ngrok_authtoken: None,\n",
      "ModelBuilder: DEBUG:         ngrok_edge: None,\n",
      "ModelBuilder: DEBUG:         tokenizer_config_path: None,\n",
      "ModelBuilder: DEBUG:         disable_grammar_support: false,\n",
      "ModelBuilder: DEBUG:         env: false,\n",
      "ModelBuilder: DEBUG:         max_client_batch_size: 4,\n",
      "ModelBuilder: DEBUG:         lora_adapters: None,\n",
      "ModelBuilder: DEBUG:         disable_usage_stats: false,\n",
      "ModelBuilder: DEBUG:         disable_crash_reports: false,\n",
      "ModelBuilder: DEBUG:     }\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:50.653403Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:50.653481Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_input_tokens` to 4095\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:50.653493Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_total_tokens` to 4096\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:50.653495Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_batch_prefill_tokens` to 4145\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:50.653497Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using default cuda graphs [1, 2, 4, 8, 16, 32]\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:50.653501Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Sharding model on 2 processes\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:50.653587Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting check and download process for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:53.715387Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Files are already present on the host. Skipping download.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:54.357426Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Successfully downloaded weights for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:54.357675Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:47:54.357710Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:02.644421Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-0\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:02.666608Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.307597897s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:02.696204Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-1\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:02.767107Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.408427457s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:02.864514Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting Webserver\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:02.930842Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m228:\u001b[0m Using the Hugging Face API\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:02.930895Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m \u001b[2m/usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/hf-hub-0.3.2/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m55:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:03.118702Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m577:\u001b[0m Serving revision 01c7f73d771dfac7d292323805ebc428287df4f9 of model meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:03.168273Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m342:\u001b[0m Overriding LlamaTokenizer with TemplateProcessing to follow python override defined in https://github.com/huggingface/transformers/blob/4aa17d00690b7f82c95bb2949ea57e22c35b4336/src/transformers/models/llama/tokenization_llama_fast.py#L203-L205\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:03.168306Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m357:\u001b[0m Using config Some(Llama)\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:03.168312Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m384:\u001b[0m Invalid hostname, defaulting to 0.0.0.0\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:03.312042Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1572:\u001b[0m Warming up model\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:05.334019Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Cuda Graphs are enabled for sizes [32, 16, 8, 4, 2, 1]\n",
      "ModelBuilder: DEBUG:     Container logging done. All container logs processed.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:06.247280Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1599:\u001b[0m Using scheduler V3\n",
      "ModelBuilder: DEBUG:     Ping health check has passed. Returned [{'generated_text': ' eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\\nThe diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The shell is oval in shape, with a high, rounded keel running down the middle of the back. The shell is dark brown to black, with a yellowish-brown to orange-brown stripe running down the middle of the back. The head is small and triangular in shape, with a pointed sn'}]\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     =============== Running Serial Benchmark.... ================\n",
      "ModelBuilder: INFO:     Invocation: 0 => Latency: 2.3142094879876822, Tokens/s: 37.7018590809893\n",
      "ModelBuilder: INFO:     Invocation: 1 => Latency: 2.313949064933695, Tokens/s: 37.70610223112241\n",
      "ModelBuilder: INFO:     Invocation: 2 => Latency: 2.315693630021997, Tokens/s: 37.677695731784354\n",
      "ModelBuilder: INFO:     Invocation: 3 => Latency: 2.311019757995382, Tokens/s: 37.75389617425086\n",
      "ModelBuilder: INFO:     Invocation: 4 => Latency: 2.3716507359640673, Tokens/s: 36.78872216592769\n",
      "ModelBuilder: INFO:     Invocation: 5 => Latency: 2.307450736989267, Tokens/s: 37.81229154813624\n",
      "ModelBuilder: INFO:     Invocation: 6 => Latency: 2.3083057049661875, Tokens/s: 37.798286341487014\n",
      "ModelBuilder: INFO:     Invocation: 7 => Latency: 2.3077682569855824, Tokens/s: 37.807089050599195\n",
      "ModelBuilder: INFO:     Invocation: 8 => Latency: 2.3088418419938534, Tokens/s: 37.78950918727861\n",
      "ModelBuilder: INFO:     Invocation: 9 => Latency: 2.311560246045701, Tokens/s: 37.74506857403145\n",
      "ModelBuilder: INFO:     ================ Completed Serial Benchmark =================\n",
      "\n",
      "ModelBuilder: INFO:     ============= Running Concurrent Benchmark.... ==============\n",
      "ModelBuilder: INFO:     User: 0 => latency: 3.3375370639842004 seconds\n",
      "ModelBuilder: INFO:     User: 1 => latency: 3.335724371019751 seconds\n",
      "ModelBuilder: INFO:     User: 2 => latency: 3.3362395080039278 seconds\n",
      "ModelBuilder: INFO:     User: 3 => latency: 3.334375594044104 seconds\n",
      "ModelBuilder: INFO:     User: 4 => latency: 3.334448324982077 seconds\n",
      "ModelBuilder: INFO:     User: 5 => latency: 3.3360336349578574 seconds\n",
      "ModelBuilder: INFO:     User: 6 => latency: 3.3319754929980263 seconds\n",
      "ModelBuilder: INFO:     User: 7 => latency: 3.3311557029373944 seconds\n",
      "ModelBuilder: INFO:     User: 8 => latency: 3.330547326011583 seconds\n",
      "ModelBuilder: INFO:     User: 9 => latency: 3.3238087189383805 seconds\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     Model Latencies for Queued Requests:\n",
      "ModelBuilder: INFO:     queued request 0: xxxx\n",
      "ModelBuilder: INFO:     queued request 1: xxxx\n",
      "ModelBuilder: INFO:     queued request 2: xxxx\n",
      "ModelBuilder: INFO:     queued request 3: xxxx\n",
      "ModelBuilder: INFO:     queued request 4: xxxx\n",
      "ModelBuilder: INFO:     queued request 5: xxxx\n",
      "ModelBuilder: INFO:     queued request 6: xxxx\n",
      "ModelBuilder: INFO:     queued request 7: xxxx\n",
      "ModelBuilder: INFO:     queued request 8: xxxx\n",
      "ModelBuilder: INFO:     queued request 9: xxxx\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     Concurrent Benchmark Metrics => throughput/s: 0.3000133889566248, standard deviation: 0.003824918732615323\n",
      "ModelBuilder: INFO:     ============== Completed Concurrent Benchmark ===============\n",
      "\n",
      "ModelBuilder: INFO:     Average latency: 2.3170449463883416, throughput/s: 0.3000133889566248 for configuration: {'HUGGING_FACE_HUB_TOKEN': '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', 'HF_TOKEN': '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', 'HF_MODEL_ID': 'meta-llama/Llama-2-7b-hf', 'SHARDED': 'true', 'NUM_SHARD': '2', 'DTYPE': 'bfloat16', 'MODEL_LOADING_TIMEOUT': '1800'}\n",
      "ModelBuilder: INFO:     Trying num shard: 1, dtype: bfloat16...\n",
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: INFO:     Pulling image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0 from repository...\n",
      "ModelBuilder: DEBUG:     Stopping currently running container...\n",
      "ModelBuilder: INFO:     Waiting for model server TGI to start up...\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:55.636228Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Args {\n",
      "ModelBuilder: DEBUG:         model_id: \"meta-llama/Llama-2-7b-hf\",\n",
      "ModelBuilder: DEBUG:         revision: None,\n",
      "ModelBuilder: DEBUG:         validation_workers: 2,\n",
      "ModelBuilder: DEBUG:         sharded: Some(\n",
      "ModelBuilder: DEBUG:             false,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         num_shard: Some(\n",
      "ModelBuilder: DEBUG:             1,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         quantize: None,\n",
      "ModelBuilder: DEBUG:         speculate: None,\n",
      "ModelBuilder: DEBUG:         dtype: Some(\n",
      "ModelBuilder: DEBUG:             BFloat16,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         trust_remote_code: false,\n",
      "ModelBuilder: DEBUG:         max_concurrent_requests: 128,\n",
      "ModelBuilder: DEBUG:         max_best_of: 2,\n",
      "ModelBuilder: DEBUG:         max_stop_sequences: 4,\n",
      "ModelBuilder: DEBUG:         max_top_n_tokens: 5,\n",
      "ModelBuilder: DEBUG:         max_input_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_input_length: None,\n",
      "ModelBuilder: DEBUG:         max_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         waiting_served_ratio: 0.3,\n",
      "ModelBuilder: DEBUG:         max_batch_prefill_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_batch_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_waiting_tokens: 20,\n",
      "ModelBuilder: DEBUG:         max_batch_size: None,\n",
      "ModelBuilder: DEBUG:         cuda_graphs: None,\n",
      "ModelBuilder: DEBUG:         hostname: \"ip-172-16-34-227.ec2.internal\",\n",
      "ModelBuilder: DEBUG:         port: 8080,\n",
      "ModelBuilder: DEBUG:         shard_uds_path: \"/tmp/text-generation-server\",\n",
      "ModelBuilder: DEBUG:         master_addr: \"localhost\",\n",
      "ModelBuilder: DEBUG:         master_port: 29500,\n",
      "ModelBuilder: DEBUG:         huggingface_hub_cache: Some(\n",
      "ModelBuilder: DEBUG:             \"/opt/ml/model/\",\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         weights_cache_override: None,\n",
      "ModelBuilder: DEBUG:         disable_custom_kernels: false,\n",
      "ModelBuilder: DEBUG:         cuda_memory_fraction: 1.0,\n",
      "ModelBuilder: DEBUG:         rope_scaling: None,\n",
      "ModelBuilder: DEBUG:         rope_factor: None,\n",
      "ModelBuilder: DEBUG:         json_output: false,\n",
      "ModelBuilder: DEBUG:         otlp_endpoint: None,\n",
      "ModelBuilder: DEBUG:         otlp_service_name: \"text-generation-inference.router\",\n",
      "ModelBuilder: DEBUG:         cors_allow_origin: [],\n",
      "ModelBuilder: DEBUG:         watermark_gamma: None,\n",
      "ModelBuilder: DEBUG:         watermark_delta: None,\n",
      "ModelBuilder: DEBUG:         ngrok: false,\n",
      "ModelBuilder: DEBUG:         ngrok_authtoken: None,\n",
      "ModelBuilder: DEBUG:         ngrok_edge: None,\n",
      "ModelBuilder: DEBUG:         tokenizer_config_path: None,\n",
      "ModelBuilder: DEBUG:         disable_grammar_support: false,\n",
      "ModelBuilder: DEBUG:         env: false,\n",
      "ModelBuilder: DEBUG:         max_client_batch_size: 4,\n",
      "ModelBuilder: DEBUG:         lora_adapters: None,\n",
      "ModelBuilder: DEBUG:         disable_usage_stats: false,\n",
      "ModelBuilder: DEBUG:         disable_crash_reports: false,\n",
      "ModelBuilder: DEBUG:     }\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:55.636303Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:55.636355Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_input_tokens` to 4095\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:55.636367Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_total_tokens` to 4096\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:55.636369Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_batch_prefill_tokens` to 4145\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:55.636371Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using default cuda graphs [1, 2, 4, 8, 16, 32]\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:55.636465Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting check and download process for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:58.705292Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Files are already present on the host. Skipping download.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:59.340185Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Successfully downloaded weights for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:48:59.340414Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:08.781682Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-0\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:08.851866Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 9.510442778s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:08.948717Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting Webserver\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:09.015738Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m228:\u001b[0m Using the Hugging Face API\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:09.015789Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m \u001b[2m/usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/hf-hub-0.3.2/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m55:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:09.198099Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m577:\u001b[0m Serving revision 01c7f73d771dfac7d292323805ebc428287df4f9 of model meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:09.247844Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m342:\u001b[0m Overriding LlamaTokenizer with TemplateProcessing to follow python override defined in https://github.com/huggingface/transformers/blob/4aa17d00690b7f82c95bb2949ea57e22c35b4336/src/transformers/models/llama/tokenization_llama_fast.py#L203-L205\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:09.247877Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m357:\u001b[0m Using config Some(Llama)\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:09.247884Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m384:\u001b[0m Invalid hostname, defaulting to 0.0.0.0\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:09.389706Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1572:\u001b[0m Warming up model\n",
      "ModelBuilder: DEBUG:     Container logging done. All container logs processed.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T02:49:11.276171Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Cuda Graphs are enabled for sizes [32, 16, 8, 4, 2, 1]\n",
      "ModelBuilder: DEBUG:     Ping health check has passed. Returned [{'generated_text': ' eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\\nThe diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The carapace is oval in shape, with a high, rounded posterior margin. The plastron is small and triangular in shape. The head is large and broad, with a short snout. The eyes are small and the nostrils are located on the top of the head. The limbs are short and'}]\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     =============== Running Serial Benchmark.... ================\n",
      "ModelBuilder: INFO:     Invocation: 0 => Latency: 3.912169707007706, Tokens/s: 22.078541185286536\n",
      "ModelBuilder: INFO:     Invocation: 1 => Latency: 3.910279083997011, Tokens/s: 22.089216177303935\n",
      "ModelBuilder: INFO:     Invocation: 2 => Latency: 3.9108516999986023, Tokens/s: 22.085981935860893\n",
      "ModelBuilder: INFO:     Invocation: 3 => Latency: 3.908296062028967, Tokens/s: 22.100423977389003\n",
      "ModelBuilder: INFO:     Invocation: 4 => Latency: 3.911205688957125, Tokens/s: 22.083983014207273\n",
      "ModelBuilder: INFO:     Invocation: 5 => Latency: 3.9100162939867005, Tokens/s: 22.090700781180374\n",
      "ModelBuilder: INFO:     Invocation: 6 => Latency: 3.906074705068022, Tokens/s: 22.11299233164457\n",
      "ModelBuilder: INFO:     Invocation: 7 => Latency: 3.910288123995997, Tokens/s: 22.089165110352983\n",
      "ModelBuilder: INFO:     Invocation: 8 => Latency: 3.9073485360713676, Tokens/s: 22.105783295913884\n",
      "ModelBuilder: INFO:     Invocation: 9 => Latency: 3.9051848279777914, Tokens/s: 22.118031234062556\n",
      "ModelBuilder: INFO:     ================ Completed Serial Benchmark =================\n",
      "\n",
      "ModelBuilder: INFO:     ============= Running Concurrent Benchmark.... ==============\n",
      "ModelBuilder: INFO:     User: 0 => latency: 4.37348833901342 seconds\n",
      "ModelBuilder: INFO:     User: 1 => latency: 4.37080914399121 seconds\n",
      "ModelBuilder: INFO:     User: 2 => latency: 4.37326160504017 seconds\n",
      "ModelBuilder: INFO:     User: 3 => latency: 4.370078054023907 seconds\n",
      "ModelBuilder: INFO:     User: 4 => latency: 4.369123592041433 seconds\n",
      "ModelBuilder: INFO:     User: 5 => latency: 4.369587138062343 seconds\n",
      "ModelBuilder: INFO:     User: 6 => latency: 4.369092431967147 seconds\n",
      "ModelBuilder: INFO:     User: 7 => latency: 4.36854413500987 seconds\n",
      "ModelBuilder: INFO:     User: 8 => latency: 4.363758573075756 seconds\n",
      "ModelBuilder: INFO:     User: 9 => latency: 4.3604681809665635 seconds\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     Model Latencies for Queued Requests:\n",
      "ModelBuilder: INFO:     queued request 0: xxxxx\n",
      "ModelBuilder: INFO:     queued request 1: xxxxx\n",
      "ModelBuilder: INFO:     queued request 2: xxxxx\n",
      "ModelBuilder: INFO:     queued request 3: xxxxx\n",
      "ModelBuilder: INFO:     queued request 4: xxxxx\n",
      "ModelBuilder: INFO:     queued request 5: xxxxx\n",
      "ModelBuilder: INFO:     queued request 6: xxxxx\n",
      "ModelBuilder: INFO:     queued request 7: xxxxx\n",
      "ModelBuilder: INFO:     queued request 8: xxxxx\n",
      "ModelBuilder: INFO:     queued request 9: xxxxx\n",
      "ModelBuilder: INFO:     \n",
      "ModelBuilder: INFO:     Concurrent Benchmark Metrics => throughput/s: 0.22889470012355087, standard deviation: 0.003785003817352275\n",
      "ModelBuilder: INFO:     ============== Completed Concurrent Benchmark ===============\n",
      "\n",
      "ModelBuilder: INFO:     Average latency: 3.909171472908929, throughput/s: 0.22889470012355087 for configuration: {'HUGGING_FACE_HUB_TOKEN': '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', 'HF_TOKEN': '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', 'HF_MODEL_ID': 'meta-llama/Llama-2-7b-hf', 'SHARDED': 'false', 'NUM_SHARD': '1', 'DTYPE': 'bfloat16', 'MODEL_LOADING_TIMEOUT': '1800'}\n",
      "ModelBuilder: INFO:     \n",
      "================================================================== Benchmark Results ==================================================================\n",
      "   AverageLatency (Serial)  P90_Latency (Serial)  AverageTokensPerSecond (Serial)  ThroughputPerSecond (Concurrent)  StandardDeviationResponse (Concurrent) NumShard     DType\n",
      "0                 1.439821              1.441343                        54.526305                          0.425582                                0.003902        4  bfloat16\n",
      "1                 2.317045              2.321289                        37.658052                          0.300013                                0.003825        2  bfloat16\n",
      "2                 3.909171              3.911302                        22.095482                          0.228895                                0.003785        1  bfloat16\n",
      "=======================================================================================================================================================\n",
      "\n",
      "ModelBuilder: INFO:     Model Configuration: {'HUGGING_FACE_HUB_TOKEN': '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', 'HF_TOKEN': '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', 'HF_MODEL_ID': 'meta-llama/Llama-2-7b-hf', 'SHARDED': 'true', 'NUM_SHARD': '4', 'DTYPE': 'bfloat16', 'MODEL_LOADING_TIMEOUT': '1800'} was most performant with avg latency: 1.4398208630969749, p90 latency: 1.4413428998086601, average tokens per second: 54.526304701535786, throughput/s: 0.4255817023001885, standard deviation of request 0.003902175776030024\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 614 ms, sys: 207 ms, total: 821 ms\n",
      "Wall time: 7min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tuned_model = model.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58184f43-61b4-4299-b873-bc2cf6ff5fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: INFO:     Pulling image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0 from repository...\n",
      "ModelBuilder: DEBUG:     Stopping currently running container...\n",
      "ModelBuilder: INFO:     Waiting for model server TGI to start up...\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559651Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Args {\n",
      "ModelBuilder: DEBUG:         model_id: \"meta-llama/Llama-2-7b-hf\",\n",
      "ModelBuilder: DEBUG:         revision: None,\n",
      "ModelBuilder: DEBUG:         validation_workers: 2,\n",
      "ModelBuilder: DEBUG:         sharded: Some(\n",
      "ModelBuilder: DEBUG:             true,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         num_shard: Some(\n",
      "ModelBuilder: DEBUG:             4,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         quantize: None,\n",
      "ModelBuilder: DEBUG:         speculate: None,\n",
      "ModelBuilder: DEBUG:         dtype: Some(\n",
      "ModelBuilder: DEBUG:             BFloat16,\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         trust_remote_code: false,\n",
      "ModelBuilder: DEBUG:         max_concurrent_requests: 128,\n",
      "ModelBuilder: DEBUG:         max_best_of: 2,\n",
      "ModelBuilder: DEBUG:         max_stop_sequences: 4,\n",
      "ModelBuilder: DEBUG:         max_top_n_tokens: 5,\n",
      "ModelBuilder: DEBUG:         max_input_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_input_length: None,\n",
      "ModelBuilder: DEBUG:         max_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         waiting_served_ratio: 0.3,\n",
      "ModelBuilder: DEBUG:         max_batch_prefill_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_batch_total_tokens: None,\n",
      "ModelBuilder: DEBUG:         max_waiting_tokens: 20,\n",
      "ModelBuilder: DEBUG:         max_batch_size: None,\n",
      "ModelBuilder: DEBUG:         cuda_graphs: None,\n",
      "ModelBuilder: DEBUG:         hostname: \"ip-172-16-34-227.ec2.internal\",\n",
      "ModelBuilder: DEBUG:         port: 8080,\n",
      "ModelBuilder: DEBUG:         shard_uds_path: \"/tmp/text-generation-server\",\n",
      "ModelBuilder: DEBUG:         master_addr: \"localhost\",\n",
      "ModelBuilder: DEBUG:         master_port: 29500,\n",
      "ModelBuilder: DEBUG:         huggingface_hub_cache: Some(\n",
      "ModelBuilder: DEBUG:             \"/opt/ml/model/\",\n",
      "ModelBuilder: DEBUG:         ),\n",
      "ModelBuilder: DEBUG:         weights_cache_override: None,\n",
      "ModelBuilder: DEBUG:         disable_custom_kernels: false,\n",
      "ModelBuilder: DEBUG:         cuda_memory_fraction: 1.0,\n",
      "ModelBuilder: DEBUG:         rope_scaling: None,\n",
      "ModelBuilder: DEBUG:         rope_factor: None,\n",
      "ModelBuilder: DEBUG:         json_output: false,\n",
      "ModelBuilder: DEBUG:         otlp_endpoint: None,\n",
      "ModelBuilder: DEBUG:         otlp_service_name: \"text-generation-inference.router\",\n",
      "ModelBuilder: DEBUG:         cors_allow_origin: [],\n",
      "ModelBuilder: DEBUG:         watermark_gamma: None,\n",
      "ModelBuilder: DEBUG:         watermark_delta: None,\n",
      "ModelBuilder: DEBUG:         ngrok: false,\n",
      "ModelBuilder: DEBUG:         ngrok_authtoken: None,\n",
      "ModelBuilder: DEBUG:         ngrok_edge: None,\n",
      "ModelBuilder: DEBUG:         tokenizer_config_path: None,\n",
      "ModelBuilder: DEBUG:         disable_grammar_support: false,\n",
      "ModelBuilder: DEBUG:         env: false,\n",
      "ModelBuilder: DEBUG:         max_client_batch_size: 4,\n",
      "ModelBuilder: DEBUG:         lora_adapters: None,\n",
      "ModelBuilder: DEBUG:         disable_usage_stats: false,\n",
      "ModelBuilder: DEBUG:         disable_crash_reports: false,\n",
      "ModelBuilder: DEBUG:     }\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559721Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559775Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_input_tokens` to 4095\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559788Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_total_tokens` to 4096\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559790Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_batch_prefill_tokens` to 4145\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559793Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using default cuda graphs [1, 2, 4, 8, 16, 32]\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559796Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Sharding model on 4 processes\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:11.559876Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting check and download process for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:14.606520Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Files are already present on the host. Skipping download.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.263619Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Successfully downloaded weights for meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.263828Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.263861Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.264807Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:15.264919Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.951367Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-0\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.951410Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-1\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.960559Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-3\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.974660Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.708372387s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.975001Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.708575449s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m1\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:23.975014Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.708197365s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m3\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.015984Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-2\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.074846Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 8.808401807s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m2\u001b[0m\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.171097Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting Webserver\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.235297Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m228:\u001b[0m Using the Hugging Face API\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.235345Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m \u001b[2m/usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/hf-hub-0.3.2/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m55:\u001b[0m Token file not found \"/opt/ml/model/token\"\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.422102Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m577:\u001b[0m Serving revision 01c7f73d771dfac7d292323805ebc428287df4f9 of model meta-llama/Llama-2-7b-hf\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.471650Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m342:\u001b[0m Overriding LlamaTokenizer with TemplateProcessing to follow python override defined in https://github.com/huggingface/transformers/blob/4aa17d00690b7f82c95bb2949ea57e22c35b4336/src/transformers/models/llama/tokenization_llama_fast.py#L203-L205\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.471679Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m357:\u001b[0m Using config Some(Llama)\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.471686Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_generation_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m384:\u001b[0m Invalid hostname, defaulting to 0.0.0.0\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:24.634265Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1572:\u001b[0m Warming up model\n",
      "ModelBuilder: DEBUG:     Container logging done. All container logs processed.\n",
      "ModelBuilder: DEBUG:     \u001b[2m2024-08-11T03:10:26.897330Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Cuda Graphs are enabled for sizes [32, 16, 8, 4, 2, 1]\n",
      "ModelBuilder: DEBUG:     Ping health check has passed. Returned [{'generated_text': ' eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\\nThe diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The shell is oval in shape, with a high, domed carapace and a flattened plastron. The carapace is dark brown to black, with a yellowish-brown to orange-brown stripe running down the middle of each scute. The plastron is yellowish-'}]\n",
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "local_predictor = tuned_model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d569f5d8-0185-496a-bd8f-1a5835db8da2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the',\n",
       " 'parameters': {'max_new_tokens': 128}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_sample_input = model_builder.schema_builder.sample_input\n",
    "updated_sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ef5548-2178-48c4-83e5-6e808be1ce30",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\\nThe diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The shell is oval in shape, with a high, domed carapace and a flattened plastron. The carapace is dark brown to black, with a yellowish-brown to orange-brown stripe running down the middle of each scute. The plastron is yellowish-'}]\n",
      "CPU times: user 4.08 ms, sys: 0 ns, total: 4.08 ms\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(local_predictor.predict(updated_sample_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e1f8d-3bec-4c82-a514-fa84daaf39ee",
   "metadata": {},
   "source": [
    "### SageMaker ModelBuilder: Deploy to a SageMaker Endpoint\n",
    "\n",
    "Now we have tested the model prediction locally, we can continue to deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f2edb6f-341f-4c3d-98c4-3f642c9d4efa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: INFO:     ModelBuilder will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features. To opt out of telemetry, please disable via TelemetryOptOut in intelligent defaults. See https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk for more info.\n",
      "ModelBuilder: WARNING:     Deploying in SAGEMAKER_ENDPOINT Mode, overriding existing configurations set for LOCAL_CONTAINER mode\n",
      "ModelBuilder: DEBUG:     Uploading TGI Model Resources uncompressed to: s3://sagemaker-us-east-1-057716757052/huggingface-pytorch-tgi-inference-2024-08-11-03-10-46-380/code\n",
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2024-08-11-03-15-02-850\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574\n",
      "WARNING:sagemaker:Failed to enable live logging: An error occurred (AccessDeniedException) when calling the FilterLogEvents operation: User: arn:aws:sts::057716757052:assumed-role/gen_ai_gsmoon/SageMaker is not authorized to perform: logs:FilterLogEvents on resource: arn:aws:logs:us-east-1:057716757052:log-group:/aws/sagemaker/Endpoints/huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574:log-stream: because no identity-based policy allows the logs:FilterLogEvents action. Fallback to default logging...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelBuilder: DEBUG:     ModelBuilder metrics emitted.\n"
     ]
    }
   ],
   "source": [
    "predictor = tuned_model.deploy(mode=Mode.SAGEMAKER_ENDPOINT, role=execution_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "174f97c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the',\n",
       " 'parameters': {'max_new_tokens': 128}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec8eabf8-0563-4d3c-9b14-0f13785fdad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diamondback terrapin or simply terrapin is a species of turtle native to the brackish coastal tidal marshes of the eastern United States. The diamondback terrapin is the only species in the genus Malaclemys.\n",
      "The diamondback terrapin is a medium-sized turtle, with a carapace length of 10–15 cm (4–6 in). The shell is oval in shape, with a high, rounded keel running down the middle of the back. The shell is dark brown to black, with a yellowish-orange stripe running down the middle of the back. The head is small and triangular in shape, with a pointed snout. The eyes are\n",
      "CPU times: user 11.6 ms, sys: 1.66 ms, total: 13.2 ms\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(predictor.predict(updated_sample_input)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8f0fe",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68053e21-a69d-4b1d-8a64-7d3f13e53cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-tgi-inference-2024-08-11-03-15-02-850\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-tgi-inference-2024-08-11-03-15-03-574\n"
     ]
    }
   ],
   "source": [
    "local_predictor.delete_predictor()\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3e748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('pytorch_p310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2257b1c3513dc4782645ad49f694a4b0012bebbbbc3534a56d350db8e4f89a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
